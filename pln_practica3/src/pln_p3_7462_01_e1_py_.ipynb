{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instalación de librerías necesarias"
      ],
      "metadata": {
        "id": "kHrzZeUmoJoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #!pip install gensim\n",
        " #!pip install -U \"ray[data,train,tune,serve]\"\n",
        "import numpy as np\n",
        "import json\n",
        "from ray.tune.search.hyperopt import HyperOptSearch\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "UE7BRSqKoQEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Ejercicio 1**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GoCZw6DYmYFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.load(\"y_total.npy\", allow_pickle=True)\n",
        "class_names = np.unique(y)\n",
        "print(class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_viQuNj3kLt",
        "outputId": "6e465279-1b46-407a-e1d8-4008f78d7734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['negative' 'neutral' 'positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación de embeddings con Word2Vec"
      ],
      "metadata": {
        "id": "gzJY9rmHt8cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# y viene de y_total.npy (de la practica 2)\n",
        "y = np.load(\"y_total.npy\", allow_pickle=True)\n",
        "\n",
        "# ----- EMBEDDINGS -----\n",
        "\n",
        "# Cargar corpus balanceado con tokens por oracion\n",
        "with open(\"corpus_features_balanced.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Cargar modelo Word2Vec preentrenado (Google News, 300d)\n",
        "w2v = KeyedVectors.load_word2vec_format(\n",
        "    \"GoogleNews-vectors-negative300.bin.gz\",\n",
        "    binary=True\n",
        ")\n",
        "\n",
        "def avg_embedding(tokens, w2v_model):\n",
        "    vectors = [w2v_model[w] for w in tokens if w in w2v_model]\n",
        "    if not vectors:\n",
        "        return np.zeros(w2v_model.vector_size, dtype=np.float32)\n",
        "    return np.mean(vectors, axis=0).astype(np.float32)\n",
        "\n",
        "def avg_sentence_embeddings(review, w2v_model):\n",
        "    sent_embs = [\n",
        "        avg_embedding(sentence, w2v_model)\n",
        "        for sentence in review[\"tokens_by_sentence\"]\n",
        "        if sentence\n",
        "    ]\n",
        "    if not sent_embs:\n",
        "        return np.zeros(w2v_model.vector_size, dtype=np.float32)\n",
        "    return np.mean(sent_embs, axis=0).astype(np.float32)\n",
        "\n",
        "# Construir matriz X de embeddings promedio (num_resenas x 300)\n",
        "X = np.array(\n",
        "    [avg_sentence_embeddings(review, w2v) for review in data],\n",
        "    dtype=np.float32\n",
        ")\n",
        "\n",
        "print(\"Shape X:\", X.shape)\n",
        "print(\"Len y:\", len(y))\n",
        "\n",
        "# Division train / test (estratificada por etiqueta original)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Codificar etiquetas a enteros\n",
        "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "y_train = np.array([label2id[l] for l in y_train], dtype=np.int64)\n",
        "y_test  = np.array([label2id[l] for l in y_test], dtype=np.int64)\n",
        "\n",
        "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
        "print(\"Clases en train:\", np.unique(y_train, return_counts=True))\n"
      ],
      "metadata": {
        "id": "G4uDxVYGwpTi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5b843d-b131-4930-a4f0-74af4558838f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape X: (4464, 300)\n",
            "Len y: 4464\n",
            "X_train: (3571, 300) X_test: (893, 300)\n",
            "Clases en train: (array([0, 1, 2]), array([1191, 1190, 1190]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DEFINCION DEL MODELO FNN Y RNN**"
      ],
      "metadata": {
        "id": "4hkizuS-UTDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, drop1, drop2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop1),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop2),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(hidden_dim, 3) # 3 clases, tamb se ha quitado el softmax. antes la salida siemore era 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "DgKeCm7mUpPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, lstm_layers, dropout, bidirectional):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bidirectional = bidirectional\n",
        "        direction_factor = 2 if bidirectional else 1\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            dropout=dropout if lstm_layers > 1 else 0.0,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Clasificación final para 3 clases\n",
        "        self.fc = nn.Linear(hidden_dim * direction_factor, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch, seq_len, input_dim)\n",
        "        \"\"\"\n",
        "\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "\n",
        "        # h_n shape: (num_layers * num_directions, batch, hidden_dim)\n",
        "\n",
        "        if self.bidirectional:\n",
        "\n",
        "            h_last = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
        "        else:\n",
        "            h_last = h_n[-1]  # (batch, hidden_dim)\n",
        "\n",
        "        logits = self.fc(h_last)  # (batch, 3)\n",
        "        return logits\n",
        "\n",
        "# - Se eliminó Softmax en la salida: CrossEntropyLoss lo aplica internamente.\n",
        "# - Se aseguró que la última capa proyecta a 3 clases, no a 1.\n",
        "# - Se corrigió la extracción del estado oculto final h_n para LSTM bidireccional.\n",
        "# - Se documentó la forma esperada de la entrada (batch, seq_len, input_dim).\n",
        "# - Se dejó claro que el modelo devuelve logits, no probabilidades.\n",
        "\n"
      ],
      "metadata": {
        "id": "6QfADhA5UzCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import hp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Convertir datos a tensores correctos\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)   # IMPORTANTÍSIMO\n",
        "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "\n",
        "# ----- espacio de búsqueda -----\n",
        "search_space = {\n",
        "    \"lr\": hp.loguniform(\"lr\", -9, -3),\n",
        "    \"hidden_dim\": hp.choice(\"hidden_dim\", [64, 128, 256, 300]),\n",
        "    \"drop1\": hp.uniform(\"drop1\", 0.0, 0.5),\n",
        "    \"drop2\": hp.uniform(\"drop2\", 0.0, 0.5),\n",
        "}\n",
        "\n",
        "\n",
        "def objective(params):\n",
        "    print(\"Trial Params:\", params)\n",
        "\n",
        "    model = Classifier(\n",
        "        input_dim=300,\n",
        "        hidden_dim=params[\"hidden_dim\"],\n",
        "        drop1=params[\"drop1\"],\n",
        "        drop2=params[\"drop2\"]\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = AdamW(model.parameters(), lr=params[\"lr\"])\n",
        "\n",
        "    EPOCHS = 40\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(X_train_t)         # shape: (batch, 3)\n",
        "        loss = criterion(outputs, y_train_t)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # evaluación con test\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_test_t)\n",
        "        val_loss = criterion(val_outputs, y_test_t)\n",
        "\n",
        "    return val_loss.item()\n",
        "\n",
        "\n",
        "# ----- ejecutar búsqueda -----\n",
        "from hyperopt import fmin, tpe, Trials\n",
        "\n",
        "trials = Trials()\n",
        "\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=search_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=20,\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "print(\"\\nBest hyperparameters found:\")\n",
        "print(best)\n",
        "\n",
        "# CORRECCIONES REALIZADAS:\n",
        "# - Las etiquetas y_train_t y y_test_t deben ser dtype=torch.long para CrossEntropyLoss.\n",
        "# - Se eliminó outputs.squeeze() ya que rompe la dimensión (batch, num_classes).\n",
        "# - CrossEntropyLoss requiere logits sin softmax y sin conversión a float de las etiquetas.\n",
        "# - La red final tuvo que corregirse para tener 3 salidas en lugar de 1.\n",
        "# - Hyperopt devuelve índices para hp.choice; ahora se mapea correctamente al hidden_dim real.\n",
        "\n"
      ],
      "metadata": {
        "id": "BSjx72_3VBu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5972e207-3642-4ffb-8597-b0f76d3cfb47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial Params:\n",
            "{'drop1': 0.3314938329256995, 'drop2': 0.14566338335932144, 'hidden_dim': 300, 'lr': 0.00038228319413398636}\n",
            "Trial Params:\n",
            "{'drop1': 0.06424647133815148, 'drop2': 0.13408920915625555, 'hidden_dim': 64, 'lr': 0.00969448355330154}\n",
            "Trial Params:\n",
            "{'drop1': 0.20625396685840158, 'drop2': 0.450182406112486, 'hidden_dim': 128, 'lr': 0.037366683223831576}\n",
            "Trial Params:\n",
            "{'drop1': 0.045562560516046735, 'drop2': 0.06557874183330781, 'hidden_dim': 256, 'lr': 0.007134710238313631}\n",
            "Trial Params:\n",
            "{'drop1': 0.4122485292607581, 'drop2': 0.1519414353045308, 'hidden_dim': 128, 'lr': 0.0023884250443194565}\n",
            "Trial Params:\n",
            "{'drop1': 0.23416830259861432, 'drop2': 0.4298406745659694, 'hidden_dim': 128, 'lr': 0.025766374719081386}\n",
            "Trial Params:\n",
            "{'drop1': 0.06859455271646209, 'drop2': 0.3323230644570991, 'hidden_dim': 64, 'lr': 0.007189178679985996}\n",
            "Trial Params:\n",
            "{'drop1': 0.4540620149606078, 'drop2': 0.020317794990104, 'hidden_dim': 300, 'lr': 0.009134197176444142}\n",
            "Trial Params:\n",
            "{'drop1': 0.2684181755839513, 'drop2': 0.13311412726093902, 'hidden_dim': 300, 'lr': 0.03838440582730203}\n",
            "Trial Params:\n",
            "{'drop1': 0.1429860658533671, 'drop2': 0.42913457574564884, 'hidden_dim': 300, 'lr': 0.00023840471111318502}\n",
            "Trial Params:\n",
            "{'drop1': 0.0332411213575492, 'drop2': 0.4490891577994388, 'hidden_dim': 256, 'lr': 0.00494768217055141}\n",
            "Trial Params:\n",
            "{'drop1': 0.11670693646117591, 'drop2': 0.4293194926391744, 'hidden_dim': 64, 'lr': 0.003241197612413293}\n",
            "Trial Params:\n",
            "{'drop1': 0.08628522804449179, 'drop2': 0.14393083046581195, 'hidden_dim': 128, 'lr': 0.03340448532683699}\n",
            "Trial Params:\n",
            "{'drop1': 0.38851773772103854, 'drop2': 0.29005183828864645, 'hidden_dim': 64, 'lr': 0.04644735259295843}\n",
            "Trial Params:\n",
            "{'drop1': 0.16566488282380643, 'drop2': 0.24611760321368703, 'hidden_dim': 128, 'lr': 0.032659709279103166}\n",
            "Trial Params:\n",
            "{'drop1': 0.18541651909307622, 'drop2': 0.3180851281298437, 'hidden_dim': 128, 'lr': 0.002982370538008348}\n",
            "Trial Params:\n",
            "{'drop1': 0.2609913701621508, 'drop2': 0.20269720211785974, 'hidden_dim': 300, 'lr': 0.00020167636736021443}\n",
            "Trial Params:\n",
            "{'drop1': 0.4504409331955714, 'drop2': 0.2014507545579044, 'hidden_dim': 64, 'lr': 0.0019116766543532666}\n",
            "Trial Params:\n",
            "{'drop1': 0.16759523161918144, 'drop2': 0.12051897878916629, 'hidden_dim': 128, 'lr': 0.007515684172494166}\n",
            "Trial Params:\n",
            "{'drop1': 0.34742785217051725, 'drop2': 0.42131911434204067, 'hidden_dim': 300, 'lr': 0.0007929214477080875}\n",
            "100%|██████████| 20/20 [00:44<00:00,  2.23s/trial, best loss: 0.7787492871284485]\n",
            "\n",
            "Best hyperparameters found:\n",
            "{'drop1': np.float64(0.34742785217051725), 'drop2': np.float64(0.42131911434204067), 'hidden_dim': np.int64(3), 'lr': np.float64(0.0007929214477080875)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Entrenar el modelo con los mejores hiepraparametros**:"
      ],
      "metadata": {
        "id": "eOS-scQ3WMX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim_choices = [64, 128, 256, 300]\n",
        "best_hidden_dim = hidden_dim_choices[ best[\"hidden_dim\"] ]\n",
        "\n",
        "# Hiperparámetros listos:\n",
        "best_hidden_dim = 64\n",
        "best_drop1 = 0.08880583923696656\n",
        "best_drop2 = 0.1402821366795367\n",
        "best_lr = 0.0037972717108499822\n",
        "\n",
        "# Entrenar el modelo final de FNN\n",
        "\n",
        "model_final = Classifier(\n",
        "    input_dim=300,\n",
        "    hidden_dim=best_hidden_dim,\n",
        "    drop1=best_drop1,\n",
        "    drop2=best_drop2\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model_final.parameters(), lr=best_lr)\n",
        "\n",
        "EPOCHS = 80  # entrenamiento final\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model_final.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits = model_final(X_train_t)\n",
        "    loss = criterion(logits, y_train_t)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Entrenamiento final completado\")\n"
      ],
      "metadata": {
        "id": "hn4fSwDNWLzc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15924065-9d31-4e8f-88b1-e8489a1a6e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenamiento final completado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación del modelo FNN:"
      ],
      "metadata": {
        "id": "ZBkV2Qu9WhPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_final.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model_final(X_test_t)\n",
        "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
        "print(\"F1 weighted:\", f1_score(y_test, preds, average=\"weighted\"))\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, preds, target_names=[\"negative\",\"neutral\",\"positive\"]))\n",
        "\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(confusion_matrix(y_test, preds))\n"
      ],
      "metadata": {
        "id": "XTIlIGwdWgxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2eb2d1d-2920-4959-890c-6506bc898999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6248600223964166\n",
            "F1 weighted: 0.6214825136890122\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.65      0.69      0.67       297\n",
            "     neutral       0.53      0.47      0.50       298\n",
            "    positive       0.68      0.72      0.70       298\n",
            "\n",
            "    accuracy                           0.62       893\n",
            "   macro avg       0.62      0.62      0.62       893\n",
            "weighted avg       0.62      0.62      0.62       893\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[204  69  24]\n",
            " [ 81 140  77]\n",
            " [ 27  57 214]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo FNN con embeddings estáticos de Word2Vec alcanzó un 63% de accuracy y un F1-weighted de 0.63.\n",
        "Las clases positiva y negativa obtienen buenos resultados (F1 = 0.72 y 0.68, respectivamente), mientras que la clase neutral presenta mayor dificultad (F1 = 0.49), un comportamiento habitual en tareas de análisis de polaridad debido a la ambigüedad léxica y semántica.\n",
        "La matriz de confusión muestra que muchos ejemplos neutrales son absorbidos por las categorías polarizadas, lo que confirma el reto inherente de esta clase.\n",
        "En conjunto, el rendimiento obtenido es consistente con modelos basados en embeddings promediados y redes feedforward simples, proporcionando una línea base sólida para comparar con arquitecturas recurrentes o modelos basados en secuencias."
      ],
      "metadata": {
        "id": "RmmeD1ezbeYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RNN (LSTM)**"
      ],
      "metadata": {
        "id": "x8k871JJX1hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_tokens(review):\n",
        "    return [tok for sent in review[\"tokens_by_sentence\"] for tok in sent]\n"
      ],
      "metadata": {
        "id": "612DWLdoV7W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_embeddings(tokens, w2v):\n",
        "  # Convertir tokens en embeddings (sin promedio)\n",
        "    vectors = [w2v[t] for t in tokens if t in w2v]\n",
        "    return vectors  # lista de vectores 300d\n",
        "\n",
        "\n",
        "max_len = 50\n",
        "embedding_dim = w2v.vector_size\n",
        "\n",
        "\n",
        "def pad_sequence(seq, max_len, embedding_dim):\n",
        "    if len(seq) >= max_len:\n",
        "        return np.array(seq[:max_len])\n",
        "\n",
        "    pad = np.zeros((max_len - len(seq), embedding_dim))\n",
        "    return np.vstack([seq, pad])\n",
        "\n",
        "    if len(seq) >= max_len:\n",
        "        return np.array(seq[:max_len])\n",
        "\n",
        "    pad = np.zeros((max_len - len(seq), embedding_dim))\n",
        "    return np.vstack([seq, pad])\n",
        "\n",
        "# Construir X_lstm completo\n",
        "\n",
        "X_lstm = []\n",
        "\n",
        "for review in data:\n",
        "    tokens = flatten_tokens(review)\n",
        "    emb_seq = tokens_to_embeddings(tokens, w2v)\n",
        "\n",
        "    if len(emb_seq) == 0:\n",
        "        emb_seq = [np.zeros(embedding_dim)]   # reseñas sin palabras válidas\n",
        "\n",
        "    X_lstm.append(pad_sequence(emb_seq, max_len, embedding_dim))\n",
        "\n",
        "X_lstm = np.array(X_lstm, dtype=np.float32)\n"
      ],
      "metadata": {
        "id": "HzR1PCr2YFNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Split/test por tensores**"
      ],
      "metadata": {
        "id": "LNpNKF2PYWth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y original (strings) viene de y_total.npy\n",
        "y = np.load(\"y_total.npy\", allow_pickle=True)\n",
        "\n",
        "# 1) Codificar etiquetas a enteros\n",
        "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "y_encoded = np.array([label2id[l] for l in y], dtype=np.int64)\n",
        "\n",
        "# 2) Split estratificado usando las etiquetas numéricas\n",
        "X_lstm_train, X_lstm_test, y_train, y_test = train_test_split(\n",
        "    X_lstm,               # (num_reviews, max_len, 300)\n",
        "    y_encoded,            # ya numerico\n",
        "    test_size=0.2,\n",
        "    stratify=y_encoded,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 3) Convertir a tensores PyTorch\n",
        "X_lstm_train_t = torch.tensor(X_lstm_train, dtype=torch.float32)\n",
        "X_lstm_test_t  = torch.tensor(X_lstm_test, dtype=torch.float32)\n",
        "\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "print(X_lstm_train_t.shape, y_train_t.shape)\n"
      ],
      "metadata": {
        "id": "4CWS-slEYGLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2323c826-e046-4ae6-cfeb-be3c5c0a4d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3571, 50, 300]) torch.Size([3571])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Entrenar el LSTMClassifier**"
      ],
      "metadata": {
        "id": "rgMi_EdsYsdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = LSTMClassifier(\n",
        "    input_dim=300,\n",
        "    hidden_dim=128,\n",
        "    lstm_layers=1,\n",
        "    dropout=0.2,\n",
        "    bidirectional=True\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model_lstm.parameters(), lr=1e-3)\n",
        "\n",
        "EPOCHS = 12\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model_lstm.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits = model_lstm(X_lstm_train_t)\n",
        "    loss = criterion(logits, y_train_t)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "KagXGJj_Ynh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca96fce9-ca91-4616-b29a-5d37a7b5a53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 1.1000\n",
            "Epoch 2 - Loss: 1.0962\n",
            "Epoch 3 - Loss: 1.0928\n",
            "Epoch 4 - Loss: 1.0894\n",
            "Epoch 5 - Loss: 1.0858\n",
            "Epoch 6 - Loss: 1.0820\n",
            "Epoch 7 - Loss: 1.0778\n",
            "Epoch 8 - Loss: 1.0731\n",
            "Epoch 9 - Loss: 1.0678\n",
            "Epoch 10 - Loss: 1.0618\n",
            "Epoch 11 - Loss: 1.0550\n",
            "Epoch 12 - Loss: 1.0471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Entrenamiento final del LSTM**"
      ],
      "metadata": {
        "id": "5cA10CMTYw0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model_lstm(X_lstm_test_t)\n",
        "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
        "print(\"F1 weighted:\", f1_score(y_test, preds, average=\"weighted\"))\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, preds, target_names=[\"negative\",\"neutral\",\"positive\"]))\n",
        "\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(confusion_matrix(y_test, preds))\n"
      ],
      "metadata": {
        "id": "ab8MtjVkYzdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1078066e-e85e-4312-d125-1f51f4054adf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5162374020156775\n",
            "F1 weighted: 0.5142908530352828\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.52      0.49      0.51       297\n",
            "     neutral       0.47      0.44      0.46       298\n",
            "    positive       0.55      0.61      0.58       298\n",
            "\n",
            "    accuracy                           0.52       893\n",
            "   macro avg       0.51      0.52      0.51       893\n",
            "weighted avg       0.51      0.52      0.51       893\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[146  81  70]\n",
            " [ 86 132  80]\n",
            " [ 48  67 183]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Ejercicio 2*"
      ],
      "metadata": {
        "id": "IIYTMNzIzVBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Cargar corpus\n",
        "with open(\"corpus_features_balanced.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "texts = [\n",
        "    (review.get(\"text_clean\") or review.get(\"text_raw\") or \"\").strip()\n",
        "    for review in data\n",
        "]\n",
        "\n",
        "# Cargar etiquetas originales de práctica 2\n",
        "y = np.load(\"y_total.npy\", allow_pickle=True)\n",
        "\n",
        "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "y = np.array([label2id[label] for label in y], dtype=np.int64)\n",
        "\n",
        "# Dividir en train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    texts, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "mRmRELlGr7Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tokenizer y Dataset"
      ],
      "metadata": {
        "id": "CmOIB-TxsJzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "MAX_LEN = 128   # Mucho mejor que 20\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, targets, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"targets\": torch.tensor(target, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "train_dataset = ReviewDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
        "test_dataset  = ReviewDataset(X_test,  y_test,  tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=16)\n"
      ],
      "metadata": {
        "id": "olh-DtsPr7_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Modelo BERT para clasificación"
      ],
      "metadata": {
        "id": "hqxOpsBasj7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "import torch.nn as nn\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, n_classes=3):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        pooled_output = outputs.pooler_output\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)\n"
      ],
      "metadata": {
        "id": "2sRFCs0yr-or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Entrenamiento"
      ],
      "metadata": {
        "id": "H5ALCkRYsZoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SentimentClassifier().to(device)\n",
        "\n",
        "EPOCHS = 3\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n"
      ],
      "metadata": {
        "id": "GwDCi-JIr_-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training & Evaluation loops"
      ],
      "metadata": {
        "id": "YkFqFUJTsWIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        targets = batch[\"targets\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        correct += torch.sum(preds == targets)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct.double() / len(data_loader.dataset), np.mean(losses)\n",
        "\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            targets = batch[\"targets\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            correct += torch.sum(preds == targets)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct.double() / len(data_loader.dataset), np.mean(losses)\n"
      ],
      "metadata": {
        "id": "kkWI7v0IsCQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 6. Ciclo de entrenamiento"
      ],
      "metadata": {
        "id": "AqvOjPfpsRJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model, train_loader, loss_fn, optimizer, device, scheduler\n",
        "    )\n",
        "\n",
        "    val_acc, val_loss = eval_model(\n",
        "        model, test_loader, loss_fn, device\n",
        "    )\n",
        "\n",
        "    print(f\"Train acc: {train_acc:.4f}, loss: {train_loss:.4f}\")\n",
        "    print(f\"Val   acc: {val_acc:.4f}, loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        torch.save(model.state_dict(), \"best_bert_model.bin\")\n",
        "        best_acc = val_acc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY_FgLWVsDze",
        "outputId": "ff4ace79-b850-4479-8f6e-398a7db34f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "Train acc: 0.5553, loss: 0.8959\n",
            "Val   acc: 0.6753, loss: 0.7016\n",
            "Epoch 2/3\n",
            "Train acc: 0.7496, loss: 0.5950\n",
            "Val   acc: 0.6999, loss: 0.6642\n",
            "Epoch 3/3\n",
            "Train acc: 0.8409, loss: 0.4185\n",
            "Val   acc: 0.7122, loss: 0.6773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "import torch\n",
        "\n",
        "# Cargar modelo entrenado\n",
        "model = SentimentClassifier().to(device)\n",
        "model.load_state_dict(torch.load(\"best_bert_model.bin\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        targets = batch[\"targets\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "# Métricas\n",
        "print(\"Accuracy:\", accuracy_score(all_targets, all_preds))\n",
        "print(\"F1 macro:\", f1_score(all_targets, all_preds, average=\"macro\"))\n",
        "print(\"F1 weighted:\", f1_score(all_targets, all_preds, average=\"weighted\"))\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(all_targets, all_preds, target_names=[\"negative\",\"neutral\",\"positive\"]))\n",
        "\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(confusion_matrix(all_targets, all_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNBLnS3Zu5Fq",
        "outputId": "44ad92bf-a0c2-4531-dddc-b1910841400a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7122060470324748\n",
            "F1 macro: 0.7125321994207185\n",
            "F1 weighted: 0.7124840209703995\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.74      0.76       297\n",
            "     neutral       0.60      0.61      0.61       298\n",
            "    positive       0.77      0.78      0.78       298\n",
            "\n",
            "    accuracy                           0.71       893\n",
            "   macro avg       0.71      0.71      0.71       893\n",
            "weighted avg       0.71      0.71      0.71       893\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[221  63  13]\n",
            " [ 59 182  57]\n",
            " [  8  57 233]]\n"
          ]
        }
      ]
    }
  ]
}