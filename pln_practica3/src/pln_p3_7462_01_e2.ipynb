{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *Ejercicio 2*"
      ],
      "metadata": {
        "id": "IIYTMNzIzVBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Cargar corpus\n",
        "with open(\"corpus_features_balanced.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "texts = [\n",
        "    (review.get(\"text_clean\") or review.get(\"text_raw\") or \"\").strip()\n",
        "    for review in data\n",
        "]\n",
        "\n",
        "# Cargar etiquetas originales de práctica 2\n",
        "y = np.load(\"y_total.npy\", allow_pickle=True)\n",
        "\n",
        "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "y = np.array([label2id[label] for label in y], dtype=np.int64)\n",
        "\n",
        "# Dividir en train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    texts, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "mRmRELlGr7Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "MAX_LEN = 128   # Mucho mejor que 20\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, targets, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"targets\": torch.tensor(target, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "train_dataset = ReviewDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
        "test_dataset  = ReviewDataset(X_test,  y_test,  tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=16)\n"
      ],
      "metadata": {
        "id": "olh-DtsPr7_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "import torch.nn as nn\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, n_classes=3):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        pooled_output = outputs.pooler_output\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)\n"
      ],
      "metadata": {
        "id": "2sRFCs0yr-or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SentimentClassifier().to(device)\n",
        "\n",
        "EPOCHS = 3\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n"
      ],
      "metadata": {
        "id": "GwDCi-JIr_-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        targets = batch[\"targets\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        correct += torch.sum(preds == targets)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct.double() / len(data_loader.dataset), np.mean(losses)\n",
        "\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            targets = batch[\"targets\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            correct += torch.sum(preds == targets)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct.double() / len(data_loader.dataset), np.mean(losses)\n"
      ],
      "metadata": {
        "id": "kkWI7v0IsCQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model, train_loader, loss_fn, optimizer, device, scheduler\n",
        "    )\n",
        "\n",
        "    val_acc, val_loss = eval_model(\n",
        "        model, test_loader, loss_fn, device\n",
        "    )\n",
        "\n",
        "    print(f\"Train acc: {train_acc:.4f}, loss: {train_loss:.4f}\")\n",
        "    print(f\"Val   acc: {val_acc:.4f}, loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        torch.save(model.state_dict(), \"best_bert_model.bin\")\n",
        "        best_acc = val_acc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY_FgLWVsDze",
        "outputId": "ff4ace79-b850-4479-8f6e-398a7db34f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "Train acc: 0.5553, loss: 0.8959\n",
            "Val   acc: 0.6753, loss: 0.7016\n",
            "Epoch 2/3\n",
            "Train acc: 0.7496, loss: 0.5950\n",
            "Val   acc: 0.6999, loss: 0.6642\n",
            "Epoch 3/3\n",
            "Train acc: 0.8409, loss: 0.4185\n",
            "Val   acc: 0.7122, loss: 0.6773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "import torch\n",
        "\n",
        "# Cargar modelo entrenado\n",
        "model = SentimentClassifier().to(device)\n",
        "model.load_state_dict(torch.load(\"best_bert_model.bin\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        targets = batch[\"targets\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "# Métricas\n",
        "print(\"Accuracy:\", accuracy_score(all_targets, all_preds))\n",
        "print(\"F1 macro:\", f1_score(all_targets, all_preds, average=\"macro\"))\n",
        "print(\"F1 weighted:\", f1_score(all_targets, all_preds, average=\"weighted\"))\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(all_targets, all_preds, target_names=[\"negative\",\"neutral\",\"positive\"]))\n",
        "\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(confusion_matrix(all_targets, all_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNBLnS3Zu5Fq",
        "outputId": "44ad92bf-a0c2-4531-dddc-b1910841400a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7122060470324748\n",
            "F1 macro: 0.7125321994207185\n",
            "F1 weighted: 0.7124840209703995\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.74      0.76       297\n",
            "     neutral       0.60      0.61      0.61       298\n",
            "    positive       0.77      0.78      0.78       298\n",
            "\n",
            "    accuracy                           0.71       893\n",
            "   macro avg       0.71      0.71      0.71       893\n",
            "weighted avg       0.71      0.71      0.71       893\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[221  63  13]\n",
            " [ 59 182  57]\n",
            " [  8  57 233]]\n"
          ]
        }
      ]
    }
  ]
}